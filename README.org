** 18 July 2021

[[file:figure-timing-grad-square-data.R]] makes [[file:figure-timing-grad-square-data.csv]]

[[file:figure-timing-grad-square.R]] makes

[[file:figure-timing-grad-square.png]]

#+begin_src R
> stats.dt[, .SD[which.max(N)], by=algorithm][order(N)]
                   algorithm        N       max    median       min
                      <char>    <int>     <num>     <num>     <num>
1:             Naïve\nsquare     3162 1.4948155 1.4085489 1.2182172
2:      Naïve\nsquared hinge     3162 1.5594398 1.4264545 1.3262214
3:        Functional\nsquare  3162277 0.9418847 0.9120649 0.8175275
4: Functional\nsquared hinge  3162277 2.3022204 2.1675010 2.0563932
5:                  Logistic 10000000 0.9223471 0.8526867 0.8260068
#+end_src

** 6 July 2021

[[file:figure-timing-grad-squared-hinge-data.R]] computes timings in simulated data, [[file:figure-timing-grad-squared-hinge-data.csv]]

[[file:figure-timing-grad-squared-hinge.R]] makes

[[file:figure-timing-grad-squared-hinge-sorted.png]]

Figure above shows slight difference in computation time when
predictions are sorted (all correct). Below we see that functional
approach (log-linear) is orders of magnitude faster than Naïve
approach, seemingly 10x slower than Logistic regression.

[[file:figure-timing-grad-squared-hinge.png]]

Below we show the timing stats for the largest size for each
algorithm. In 1 second: 
- Naïve can learn from ~10^3 examples, 
- Functional can learn from ~10^6 examples,
- Logistic can learn from ~10^7 examples.

#+begin_src R
    algorithm        N prediction.order       max    median       min
1:      Naïve     3162         unsorted 1.1355375 1.0555564 1.0300934
2: Functional  1000000         unsorted 0.7038154 0.6431228 0.6332610
3:   Logistic 10000000         unsorted 0.9513424 0.9233314 0.9145711
#+end_src

** 28 June 2021
Outline of paper:
- AUC can be computed via Mann-Whitney sum of indicator functions,
  over all pairs of positive and negative labels.
- Indicator functions can be relaxed to linear or squared hinge loss,
  which have gradients so are useful for learning algorithms.
- Any of these loss functions (and gradients) can be computed, for N
  labeled training examples, in log-linear O(N log N) time, by first
  sorting the predicted values (offset one class by the margin), then
  using a cumulative sum of a functional representation of the loss
  functions (constant, linear, quadratic coefficients). 
- Indicator function (AUC) uses only the constant coefficient
  (actually needs a modification if there are ties).
- Linear hinge loss uses constant and linear coefficients.
- Squared hinge loss uses all three coefficients (constant, linear,
  quadratic).
- For the special case of linear hinge loss with margin=0 we can
  interpret the loss function as Area Under the Product (AUP) of False
  Positive and False Negative functions. (only difference with our AUM
  loss function is replacing Min with Product)
- This suggests exploring generalizations of AUM using non-zero
  margin, and squared rather than linear hinge loss. 
- Novelty: faster algorithm for computing existing loss function /
  gradient in binary classification problems, new loss functions for
  changepoint detection problems.
- Experiments: Min vs Product? Linear vs quadratic hinge loss? Zero
  versus non-zero margin?
